<html>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.1/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-iYQeCzEYFbKjA/T2uDLTpkwGzCiq6soy8tYaI1GyVh/UjpbCx/TYkiZhlZB6+fzT" crossorigin="anonymous" />
<script src="dist/ort.webgpu.min.js"> </script>

<!--
<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.16.0-dev.20230910-24f0893d3c/dist/ort.webgpu.min.js"> </script>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.webgpu.min.js"></script>
    <script src="dist/ort.min.js"> </script>
    <script src="dist/ort.js"> </script>
    <script src="dist/ort.webgpu.min.js"> </script>
-->

<head>
    <title>ort-web perf</title>
</head>
<style>
    .sidebar {
        margin: 0;
        padding: 1;
        width: 200px;
        background-color: #f1f1f1;
        position: fixed;
        height: 100%;
        overflow: auto;
        overflow-y: scroll;
        scrollbar-width: 8px;

    }

    .sidebar a {
        display: block;
        color: black;
        text-decoration: none;
        overflow: hidden;
    }

    .sidebar a.active {
        background-color: #04AA6D;
        color: white;
    }

    .sidebar a:hover:not(.active) {
        background-color: #555;
        color: white;
    }

    .content {
        margin-left: 220px;
    }
</style>

<body>
    <div class="containerx" style="padding-left: 10px;">
        <h1>ort-web perf</h1>
        <div id="nav" class="sidebar d-grid gap-5 d-md-block">
        </div>
        <div class="content">
            <div class="d-grid gap-5 d-md-block">
                <button class="btn btn-primary" type="button" onclick="bench();">bench</button>
            </div>
            <div class="d-grid gap-5 d-md-block">
                <div class="progress" style="width: 50%; margin-top: 10; display: none;">
                    <div id="progress" class="progress-bar progress-bar-striped" role="progressbar" style="width: 10%"
                        aria-valuenow="0" aria-valuemin="0" aria-valuemax="100" sam></div>
                </div>
            </div>
            <p class="text-lg-start">
            <div id="results" style="font: 1em consolas;"></div>
            </p>
            <p class="text-lg-start">
            <div id="status" style="font: 1em consolas;"></div>
            </p>
        </div>
    </div>
</body>

<script>
    const models = [
        // working
        { n: "mobilenet-v2", g: "img224", m: "mobilenet_v2/model-12.onnx" },
        { n: "albert-base-v2", g: "bert64", m: "tjs/albert-base-v2/onnx/model.onnx" },
        { n: "bert-base-uncased", g: "bert64", m: "tjs/bert-base-uncased/onnx/model.onnx" },
        { n: "distilbert-base-uncased", g: "bert64", m: "tjs/distilbert-base-uncased/onnx/model.onnx" },
        { n: "roberta-base", g: "bert64", m: "tjs/roberta-base/onnx/model.onnx" },
        { n: "sentence-trans-msbert", g: "bert64", m: "tjs/sentence-transformers/msmarco-distilbert-base-v4/onnx/model.onnx" },
        { n: "nli-deberta-v3-small", g: "bert64", m: "tjs/cross-encoder/nli-deberta-v3-small/onnx/model.onnx" },
        { n: "t5-encoder", g: "t5-encoder", m: "tjs/t5-small/onnx/encoder_model.onnx" },
        { n: "t5-decoder-seq1", g: "t5-decoder", m: "tjs/t5-small/onnx/decoder_model_merged.onnx", o: "&seqlen=1" },
        { n: "t5-v1.1-encoder", g: "t5-encoder", m: "tjs/google/t5-v1_1-small/onnx/encoder_model.onnx" },
        { n: "t5-v1.1-decoder-seq1", g: "flan-t5-decoder", m: "tjs/google/t5-v1_1-small/onnx/decoder_model_merged.onnx", o: "&seqlen=1" },
        { n: "flan-t5-encoder", g: "t5-encoder", m: "tjs/google/flan-t5-small/onnx/encoder_model.onnx" },
        { n: "flan-t5-decoder-seq1", g: "flan-t5-decoder", m: "tjs/google/flan-t5-small/onnx/decoder_model_merged.onnx", o: "&seqlen=1" },

        // FIXME: use_cache_branch=false because some issue with the model on wasm and webgpu
        { n: "gpt-neo-125m", g: "llm-decoder", m: "tjs/EleutherAI/gpt-neo-125M/onnx/decoder_model_merged.onnx", o: "&use_cache_branch=1" },
        { n: "distilgpt2-seq16", g: "llm-decoder", m: "tjs/distilgpt2/onnx/decoder_model_merged.onnx", o: "&seqlen=16&use_cache_branch=1" },
        { n: "gpt2", g: "llm-decoder", m: "tjs/gpt2/onnx/decoder_model_merged.onnx", o: "&seqlen=8&use_cache_branch=1" },

        { n: "whisper-decoder", g: "whisper-decoder", m: "tjs/openai/whisper-tiny/onnx/decoder_model_merged.onnx", o: "&seqlen=1" },
        { n: "whisper-encoder", g: "whisper-encoder", m: "tjs/openai/whisper-tiny/onnx/encoder_model.onnx" },
        { n: "dino-vitb16", g: "img224", m: "tjs/facebook/dino-vitb16/onnx/model.onnx" },
        { n: "clip-vit-base-patch16", g: "clip", m: "tjs/openai/clip-vit-base-patch16/onnx/model.onnx" },
        { n: "vit-base-patch16-224", g: "img224", m: "tjs/google/vit-base-patch16-224/onnx/model.onnx" },
        { n: "vit-gpt2-image-captioning", g: "bart-large-12", m: "tjs/nlpconnect/vit-gpt2-image-captioning/onnx/decoder_model_merged.onnx" },
        { n: "sam-b-encoder", g: "sam-encoder", m: "sam/sam_vit_b-encoder.onnx", o: "&min_query_count=10" },
        { n: "sam-b-decoder", g: "sam-decoder", m: "sam/sam_vit_b-decoder.onnx" },
        { n: "sam-h-decoder", g: "sam-decoder", m: "sam/sam_vit_h-decoder.onnx" },
        { n: "sam-h-decoder-static", g: "sam-decoder", m: "sam/segment-anything-vit-h-static-shapes-static.onnx" },
        { n: "bart-large-encoder", g: "bert64", m: "tjs/facebook/bart-large-cnn/onnx/encoder_model.onnx" },
        { n: "distilbert-base-uncased-mnli", g: "bert64", m: "tjs/distilbert-base-uncased-mnli/onnx/model.onnx", o: "&seqlen=50" },
        { n: "distilbart-cnn-6-6-encoder", g: "bert64", m: "tjs/distilbart-cnn-6-6/onnx/encoder_model.onnx", o: "&seqlen=168" },
        { n: "distilbart-cnn-6-6-decoder", g: "bart-cnn", m: "tjs/distilbart-cnn-6-6/onnx/decoder_model_merged.onnx", o: "&seqlen=168" },
        { n: "vit-gpt2-image-captioning-encoder", g: "img224", m: "tjs/vit-gpt2-image-captioning/onnx/encoder_model.onnx", o: "&seqlen=168" },
        { n: "vit-gpt2-image-captioning-decoder", g: "bart-large-12", m: "tjs/vit-gpt2-image-captioning/onnx/decoder_model_merged.onnx", o: "&seqlen=168" },
        { n: "yolo-small", g: "img640x480", m: "tjs/hustvl/yolos-small/onnx/model.onnx" },
        { n: "detr-resnet-50", g: "detr", m: "tjs/facebook/detr-resnet-50/onnx/model.onnx" },
        { n: "squeezebert", g: "bert64", m: "tjs/squeezebert/squeezebert-uncased/onnx/model.onnx" },

        // https://github.com/Megvii-BaseDetection/YOLOX
        { n: "yolox-s", g: "img640x640", m: "partya/yolox/yolox_s.onnx" },
        { n: "yolox-l", g: "img640x640", m: "partya/yolox/yolox_l.onnx" },
        // https://github.com/mit-han-lab/efficientvit
        { n: "efficientvit-l1", g: "img512x512", m: "partya/efficientvit/l1.onnx" },
        // https://github.com/snap-research/EfficientFormer
        { n: "efficientformer-l1", g: "img224", m: "partya/efficientformer/efficientformer_l1.onnx" },
        { n: "efficientformer-l3", g: "img224", m: "partya/efficientformer/efficientformer_l3.onnx" },

        // https://github.com/hustvl/TopFormer
        // https://github.com/ibaiGorordo/ONNX-TopFormer-Semantic-Segmentation
        { n: "topformer", g: "img512x512", m: "partya/TopFormer/TopFormer-S_512x512_2x8_160k.onnx" },

        // https://github.com/NVlabs/SegFormer
        // https://huggingface.co/spaces/chansung/segformer-tf-transformers/blob/main/segformer-b5-finetuned-ade-640-640.onnx
        { n: "segformer", g: "img640x640", m: "partya/SegFormer/segformer-b5-finetuned-ade-640-640.onnx" },

        // https://github.com/facebookresearch/HRViT (no onnx model)
        // https://github.com/Gaurav14cs17/YOLOE (no pre-trained model)
        //

        { n: "-", g: "-", m: "p-" },
        { n: "-", g: "-", m: "p-" },
        { n: "-", g: "-", m: "p-" },

        // transformers.js demo example
        { e: "tjs-demo", n: "t5-encoder", g: "t5-encoder", m: "tjs/t5-small/onnx/encoder_model.onnx", o: "&seqlen=128" },
        { e: "tjs-demo", n: "t5-decoder-seq1", g: "t5-decoder", m: "tjs/t5-small/onnx/decoder_model_merged.onnx", o: "&seqlen=1&enc_seqlen=128" },
        { e: "tjs-demo", n: "distilgpt2", g: "llm-decoder", m: "tjs/distilgpt2/onnx/decoder_model_merged.onnx", o: "&seqlen=16" },
        { e: "tjs-demo", n: "bert-base-cased", g: "bert64", m: "tjs/bert-base-cased/onnx/model.onnx", o: "&seqlen=9" },
        { e: "tjs-demo", n: "bert-base-sentiment", g: "bert64", m: "tjs/bert-base-multilingual-uncased-sentiment/onnx/model.onnx", o: "&seqlen=63" },
        { e: "tjs-demo", n: "distilbert-base-uncased-mnli", g: "bert64", m: "tjs/distilbert-base-uncased-mnli/onnx/model.onnx", o: "&seqlen=50" },
        { e: "tjs-demo", n: "distilbert-distilled-squad", g: "bert64", m: "tjs/distilbert-base-cased-distilled-squad/onnx/model.onnx", o: "&seqlen=262" },
        { e: "tjs-demo", n: "distilbart-cnn-6-6-encoder", g: "bert64", m: "tjs/distilbart-cnn-6-6/onnx/encoder_model.onnx", o: "&seqlen=168" },
        { e: "tjs-demo", n: "distilbart-cnn-6-6-decoder", g: "bart-cnn", m: "tjs/distilbart-cnn-6-6/onnx/decoder_model_merged.onnx", o: "&seqlen=168" },
        { e: "tjs-demo", n: "whisper-decoder-seq1", g: "whisper-decoder", m: "tjs/openai/whisper-tiny/onnx/decoder_model_merged.onnx", o: "&seqlen=1" },
        { e: "tjs-demo", n: "whisper-encoder", g: "whisper-encoder", m: "tjs/openai/whisper-tiny/onnx/encoder_model.onnx" },
        { e: "tjs-demo", n: "vit-gpt2-image-captioning-encoder", g: "img224", m: "tjs/vit-gpt2-image-captioning/onnx/encoder_model.onnx", o: "&seqlen=168" },
        { e: "tjs-demo", n: "vit-gpt2-image-captioning-decoder", g: "bart-large-12", m: "tjs/vit-gpt2-image-captioning/onnx/decoder_model_merged.onnx", o: "&seqlen=168" },
        { e: "tjs-demo", n: "vit-base-patch16-224", g: "img224", m: "tjs/google/vit-base-patch16-224/onnx/model.onnx" },
        { e: "tjs-demo", n: "clip-vit-base-patch16", g: "clip", m: "tjs/openai/clip-vit-base-patch16/onnx/model.onnx" },
        { e: "tjs-demo", n: "detr-resnet-50", g: "detr", m: "tjs/facebook/detr-resnet-50/onnx/model.onnx" },

        // not working

        // to large, 2.5GB
        { e: "error", n: "sam-h-encoder", g: "sam-encoder", m: "sam/sam_vit_h-encoder.onnx", o: "&min_query_count=10" },

        // /swin/pooler/GlobalAveragePool" failed. Error: Error: Pool ops supports 2-D inputs only for now.
        { e: "error", n: "swin-tiny", g: "img224", m: "tjs/microsoft/swin-tiny-patch4-window7-224/onnx/model.onnx" },

        // Conv > 2D
        { e: "error", n: "wav2vec", g: "wav2vec", m: "tjs/facebook/wav2vec2-base-960h/onnx/model.onnx" },

        // Transpose freemem
        { e: "error", n: "mobilevit", g: "mobilevit", m: "tjs/apple/mobilevit-small/onnx/model.onnx" },

        // matmul fails
        { e: "error", n: "tiny_starcoder_py", g: "starcoder", m: "tjs/bigcode/tiny_starcoder_py/onnx/decoder_model_merged.onnx" },

        // decoder: Gather" failed. Error: Error: no GPU data for input: 1238119040
        { e: "error", n: "bart-large-decoder", g: "bart-large", m: "tjs/facebook/bart-large-cnn/onnx/decoder_model_merged.ort" },
        { e: "error", n: "bart-large-cnn", g: "bart-large", m: "tjs/facebook/bart-large-cnn/onnx/decoder_model_merged.ort" },


        // OOM
        { e: "error", n: "codegen-350M-mono", g: "llm-decoder", m: "tjs/Salesforce/codegen-350M-mono/onnx/decoder_model_merged.ort" },

        // Gather fails
        { e: "error", n: "xlm-roberta-base", g: "bert64", m: "tjs/xlm-roberta-base/onnx/model.ort" },
        { e: "error", n: "sd-unet-fp16", g: "sd-unet", m: "sd-fp16/unet/model.ort" },
        { e: "error", n: "sd-vae-fp16", g: "sd-vae", m: "sd-fp16/unet/model.ort" },
    ];

    function populateNav(config) {
        const nav = document.getElementById("nav");
        for (let i = 0; i < models.length; i++) {
            const model = models[i];
            if (model.e === undefined) {
                model.e = "default";
            }
            if (model.o === undefined) {
                model.o = "";
            }
            if (model.e != config.filter) {
                continue;
            }
            const a = document.createElement("a");
            a.href = `?filter=${config.filter}&provider=${config.provider}&name=${model.n}&gen=${model.g}&model=${model.m}${model.o}`;
            a.innerText = model.n;
            if (model.n == config.name) {
                a.className = "active";
            }
            nav.appendChild(a);
        }
    }

    function log(i) {
        console.log(i);
        document.getElementById('status').innerText += `\n[${performance.now().toFixed(3)}] ` + i;
    }

    function sleep(ms) {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    function getQueryVariables() {
        const query = window.location.search.substring(1);
        var config = {
            model: undefined,
            name: undefined,
            filter: "default",
            provider: "webgpu",
            device: "cpu",
            threads: "1",
            profiler: "0",
            gen: "img224",
            verbose: "0",
            min_query_count: "30",
            min_query_time: "10",
            seqlen: "128",
            enc_seqlen: "128",
            use_cache_branch: "1",
            io_binding: "0",
            go: 0,
            csv: 0,
            name: undefined
        };
        let vars = query.split("&");
        for (var i = 0; i < vars.length; i++) {
            let pair = vars[i].split("=");
            if (pair[0] in config) {
                config[pair[0]] = decodeURIComponent(pair[1]);
            } else if (pair[0].length > 0) {
                throw new Error("unknown argument: " + pair[0]);
            }
        }
        config.threads = parseInt(config.threads);
        config.verbose = parseInt(config.verbose);
        config.profile = parseInt(config.profile);
        config.seqlen = parseInt(config.seqlen);
        config.enc_seqlen = parseInt(config.enc_seqlen);
        config.use_cache_branch = (config.use_cache_branch == "1");
        config.io_binding = (config.io_binding == "1");
        config.go = parseInt(config.go);
        config.profiler = parseInt(config.profiler);
        config.csv = parseInt(config.csv);
        config.min_query_time = parseFloat(config.min_query_time) * 1000;
        config.min_query_count = parseInt(config.min_query_count);
        return config;
    }

    function logPercentiles(config, data) {
        const fp = 2;
        let result = {};
        const calculatePercentile = (percentile, numbers) => {
            const pos = Math.ceil(percentile / 100.0 * numbers.length) - 1;
            return numbers[pos];
        };

        data.sort(function (a, b) { return a - b }); // increasing.
        const sum = data.reduce((a, b) => a + b);
        result.avg = (sum / data.length).toFixed(fp);
        result.count = data.length;
        result.pass = 1;
        result.mem = 0;
        result.count = data.length;
        result.threads = config.threads;
        result.name = config.name;
        result.provider = config.provider;
        result.min = data[0].toFixed(fp);
        result.max = data[data.length - 1].toFixed(fp);
        result.p50 = calculatePercentile(50, data).toFixed(fp);
        result.p95 = calculatePercentile(95, data).toFixed(fp);
        result.p99 = calculatePercentile(99, data).toFixed(fp);
        return result;
    }

    async function fetchAndCache(url) {
        try {
            const cache = await caches.open("onnx");
            let cachedResponse = await cache.match(url);
            if (cachedResponse == undefined) {
                await cache.add(url);
                cachedResponse = await cache.match(url);
            } else {
                log(`from cache: ${url}`);
            }

            const data = await cachedResponse.arrayBuffer();
            return data;
        } catch (error) {
            log(`cache failed: ${url}`);
            return await fetch(url).then(response => response.arrayBuffer());
        }
    }

    function fillTensor(shape, dtype, val = 1) {
        let size = 1;
        shape.forEach(element => {
            size *= element;
        });
        switch (dtype) {
            case "float32":
                return new ort.Tensor(Float32Array.from({ length: size }, () => val), shape);
            case "int32":
                return new ort.Tensor(Int32Array.from({ length: size }, () => val), shape);
            case "int64":
                return new ort.Tensor(BigInt64Array.from({ length: size }, () => val), shape);
        }
        throw new Error(`Input tensor type ${dtype} is not supported`);
    }

    function randomTensor(shape, dtype) {
        let size = 1;
        shape.forEach(element => {
            if (element > 1) {
                size *= element;
            }
        });
        switch (dtype) {
            case "float32":
                return new ort.Tensor(Float32Array.from({ length: size }, () => Math.random()), shape);
        }
        throw new Error(`Input tensor type ${dtype} is not supported`);
    }

    function rampTensor(shape, dtype) {
        let size = 1;
        shape.forEach(element => {
            if (element > 1) {
                size *= element;
            }
        });
        switch (dtype) {
            case "float32":
                return new ort.Tensor(Float32Array.from({ length: size }, (_, i) => i), shape);
            case "int32":
                return new ort.Tensor(Int32Array.from({ length: size }, (_, i) => i), shape);
            case "int64":
                return new ort.Tensor(BigInt64Array.from({ length: size }, (_, i) => i), shape);
        }
        throw new Error(`Input tensor type ${dtype} is not supported`);
    }

    function make_inputs(gen, inputNames, config) {
        const seqlen = config.seqlen;
        const enc_seqlen = config.enc_seqlen;
        const feed = {};
        const name = inputNames[0];
        if (gen == "sd-unet") {
            feed["sample"] = rampTensor([1, 3, 512, 512], "float32");
            feed["timestep"] = fillTensor([1], "int64", 1n);
            feed["encoder_hidden_states"] = fillTensor([1, 3, 512, 512], "float32");
            return feed;
        }
        if (gen == "sd-vae") {
            feed["latent_sample"] = rampTensor([1, 3, 512, 512], "float32");
            return feed;
        }
        if (gen == "vit") {
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.includes("_values")) {
                    feed[v] = rampTensor([1, 3, 224, 224], "float32");
                } else if (v == "attention_mask") {
                    feed[v] = fillTensor([1, seqlen], "int64", 1n);
                } else {
                    feed[v] = fillTensor([1, seqlen], "int64", 49407n);
                }
            }
            return feed;
        }
        if (gen == "clip") {
            feed["input_ids"] = fillTensor([1, 77], "int64", 49407n);
            feed["pixel_values"] = fillTensor([1, 3, 224, 224], "float32", 99);
            feed["attention_mask"] = fillTensor([1, 77], "int64", 1n);
            return feed;
        }
        if (gen == "detr") {
            feed["pixel_values"] = fillTensor([1, 3, 224, 224], "float32", 99);
            feed["pixel_mask"] = fillTensor([1, 64, 64], "int64", 1n);
            return feed;
        }
        if (gen == "mobilevit") {
            feed["pixel_values"] = fillTensor([1, 3, 224, 224], "float32", 99);
            return feed;
        }
        if (gen == "wav2vec") {
            feed["input_values"] = fillTensor([1, 512], "float32");
            return feed;
        }
        if (gen == "whisper-encoder") {
            feed["input_features"] = fillTensor([1, 80, 3000], "float32");
            return feed;
        }
        if (gen == "whisper-decoder") {
            feed["input_ids"] = fillTensor([1, seqlen], "int64", 1n);
            feed["encoder_hidden_states"] = fillTensor([1, 1500, 384], "float32");
            // past_key_values.0.encoder.value[FLOAT, batch_sizex6xencoder_sequence_length_outx64]
            // past_key_values.1.decoder.key[FLOAT, batch_sizex6xpast_decoder_sequence_lengthx64]
            const decoder_shape = [1, 6, seqlen, 64];
            const encoder_shape = [1, 6, 1500, 64];
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values.")) {
                    if (v.includes("decoder")) {
                        feed[v] = fillTensor(decoder_shape, "float32", 1);
                    } else if (v.includes("encoder")) {
                        feed[v] = fillTensor(encoder_shape, "float32", 1);
                    }
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            return feed;
        }
        if (gen == "img512") {
            feed[name] = rampTensor([1, 3, 512, 512], "float32");
            return feed;
        }
        if (gen == "img224") {
            feed[name] = rampTensor([1, 3, 224, 224], "float32");
            return feed;
        }
        if (gen == "img640x480") {
            feed[name] = randomTensor([1, 3, 640, 480], "float32");
            return feed;
        }
        if (gen == "img640x640") {
            feed[name] = randomTensor([1, 3, 640, 640], "float32");
            return feed;
        }
        if (gen == "img512x512") {
            feed[name] = randomTensor([1, 3, 512, 512], "float32");
            return feed;
        }
        if (gen == "img224nhwc") {
            feed[name] = rampTensor([1, 224, 224], "float32");
            return feed;
        }
        if (gen == "bert" || gen == "bert64") {
            const dtype = (gen == "bert") ? "int32" : "int64";
            const val = (gen == "bert") ? 99 : 99n;
            const one = (gen == "bert") ? 1 : 1n;

            for (var k in inputNames) {
                const v = inputNames[k];
                if (v === "input_ids") {
                    feed[v] = fillTensor([1, seqlen], dtype, val);
                }
                if (v === "input_mask" || v === "attention_mask") {
                    feed[v] = fillTensor([1, seqlen], dtype, one);
                }
                if (v === "token_type_ids" || v == "segment_ids") {
                    feed[v] = fillTensor([1, seqlen], dtype, one);
                }
            }
            return feed;
        }
        if (gen == "llm-decoder") {
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values")) {
                    feed[v] = fillTensor([1, 12, seqlen, 64], "float32", .5);
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            feed["input_ids"] = fillTensor([1, seqlen + 1], "int64", 99n);
            feed["attention_mask"] = fillTensor([1, seqlen + 1], "int64", 1n);
            return feed;
        }
        if (gen == "starcoder") {
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values")) {
                    feed[v] = fillTensor([1, seqlen, 128], "float32", 1.);
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            feed["input_ids"] = fillTensor([1, seqlen], "int64", 99n);
            feed["attention_mask"] = fillTensor([1, seqlen], "int64", 1n);
            return feed;
        }
        if (gen == "bart-large" || gen == "bart-large-12") {
            const kvdim = (gen == "bart-large") ? 16 : 12;
            const hiddendim = (gen == "bart-large") ? 1024 : 768;
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values")) {
                    feed[v] = fillTensor([1, kvdim, seqlen, 64], "float32", 1.);
                }
                if (v.startsWith("encoder_attention_mask")) {
                    feed["encoder_attention_mask"] = fillTensor([1, enc_seqlen], "int64", 1n);
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            feed["input_ids"] = fillTensor([1, seqlen], "int64", 99n);
            feed["encoder_hidden_states"] = fillTensor([1, enc_seqlen, hiddendim], "float32", 1);
            return feed;
        }
        if (gen == "bart-cnn") {
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values")) {
                    feed[v] = fillTensor([1, 16, seqlen, 64], "float32", 1.);
                }
                if (v == "encoder_attention_mask") {
                    feed["encoder_attention_mask"] = fillTensor([1, enc_seqlen], "int64", 1n);
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            feed["input_ids"] = fillTensor([1, seqlen], "int64", 99n);
            feed["encoder_hidden_states"] = fillTensor([1, seqlen, 1024], "float32", 1);
            return feed;
        }
        if (gen == "vae-decoder") {
            feed[name] = fillTensor([1, 4, 64, 64], "float32", 1);
            return feed;
        }
        if (gen == "sam-decoder") {
            feed["image_embeddings"] = fillTensor([1, 256, 64, 64], "float32", 0.5);
            feed["point_coords"] = new ort.Tensor(new Float32Array([327.1111, 426.875, 241.77777, 341.5, 398.22223, 498.02084]), [1, 3, 2]);
            feed["point_labels"] = new ort.Tensor(new Float32Array([0., 2., 3.]), [1, 3]);
            feed["mask_input"] = fillTensor([1, 1, 256, 256], "float32", 0.);
            feed["has_mask_input"] = fillTensor([1], "float32", 1.);
            if (inputNames.includes("orig_im_size")) {
                feed["orig_im_size"] = new ort.Tensor(new Float32Array([512., 512.]), [2]);
            }
            return feed;
        }
        if (gen == "sam-encoder") {
            feed["input_image"] = fillTensor([224, 224, 3], "float32", 1.);
            return feed;
        }
        if (gen == "t5-encoder") {
            feed['input_ids'] = fillTensor([1, seqlen], "int64", 99n);
            feed['attention_mask'] = fillTensor([1, seqlen], "int64", 1n);
            return feed;
        }
        if (gen == "t5-decoder" || gen == "flan-t5-decoder") {
            feed['input_ids'] = fillTensor([1, seqlen], "int64", 99n);
            feed['encoder_hidden_states'] = fillTensor([1, seqlen, 512], "float32", 1);
            const encoder_shape = (gen == "t5-decoder") ? [1, 8, enc_seqlen, 64] : [1, 6, enc_seqlen, 64];
            const decoder_shape = (gen == "t5-decoder") ? [1, 8, seqlen, 64] : [1, 6, seqlen, 64];
            for (var k in inputNames) {
                const v = inputNames[k];
                if (v.startsWith("past_key_values.")) {
                    if (v.includes("decoder")) {
                        feed[v] = fillTensor(decoder_shape, "float32", 1);
                    } else if (v.includes("encoder")) {
                        feed[v] = fillTensor(encoder_shape, "float32", 1);
                    }
                }
                if (v == "encoder_attention_mask") {
                    feed['encoder_attention_mask'] = fillTensor([1, enc_seqlen], "int64", 1n);
                }
            }
            feed['use_cache_branch'] = new ort.Tensor("bool", [config.use_cache_branch], [1]);
            return feed;
        }
        throw new Error(`unknown gendata ${gen}`);
    }

    const progress = document.getElementById('progress');
    const config = getQueryVariables();

    populateNav(config);

    if (config.model === undefined && config.name !== undefined) {
        for (let i = 0; i < models.length; i++) {
            const model = models[i];
            if (model.n == config.name && model.e == config.filter) {
                config.model = model.m;
                config.gen = model.g;
                break;
            }
        }
        if (config.model === undefined) {
            throw new Error(`${config.filter} / ${config.name} not defined`);
        }
    }

    ort.env.wasm.numThreads = config.threads;
    ort.env.wasm.simd = true;
    ort.env.wasm.proxy = (config.provider == "webnn");

    if (config.provider == "webgpu") {
        if (!("gpu" in navigator)) {
            throw new Error("webgpu is NOT supported");
        }
        navigator.gpu.requestAdapter().then(adapter => {
            adapter.requestAdapterInfo().then(info => {
                log(`GPU: ${info.description}`);
            });
        });
    }
    if (config.provider == "webnn") {
        if (!("ml" in navigator)) {
            throw new Error("webnn is NOT supported");
        }
    }
    const type_to_func = {
        "float32": Float32Array,
        "int32": Int32Array,
        "BigInt64Array": BigInt64Array,
        "int64": BigInt64Array,
    };

    const rows = ["name", "provider", "pass", "threads", "avg", "min", "max", "p50", "p95", "p99", "count", "mem", "tag", "platform", "fist_run", "load"];

    async function bench() {
        try {
            document.getElementById('status').innerText = "";
            const use_proxy = ort.env.wasm.proxy;

            const opt = {
                executionProviders: [config.provider],
                enableMemPattern: false,
                enableCpuMemArena: false,
                extra: {
                    session: {
                        disable_prepacking: "1",
                        use_device_allocator_for_initializers: "1",
                        use_ort_model_bytes_directly: "1",
                        use_ort_model_bytes_for_initializers: "1"
                    }
                }
            };
            // opt.getGraphOptimzationLevel = "disable";
            if (config.verbose) {
                opt.logSeverityLevel = 0;
                opt.logVerbosityLevel = 0;
                ort.env.debug = true;
                ort.env.logLevel = "verbose";
            }

            if (config.provider == "webnn") {
                opt.executionProviders = [{
                    name: "webnn",
                    deviceType: config.device,
                    powerPreference: 'default'
                }];
            }

            function clone(x) {
                if (use_proxy) {
                    let feed = {};
                    for (const [key, value] of Object.entries(x)) {
                        let func = type_to_func[value.type];
                        feed[key] = new ort.Tensor(func.from(value.data), value.dims);
                    }
                    return feed;
                } else {
                    return x;
                }
            }

            function copy_kv_caches(feed, outputs) {
                for (const [name, t1] of Object.entries(outputs)) {
                    if (name.startsWith('present')) {
                        let t = t1;
                        let newName = name.replace('present', 'past_key_values');
                        if (t.dims[0] == 0) {
                            // Optimization introduced by optimum to reuse past key values. So, we just replace the constant
                            t.dispose();
                        } else {
                            feed[newName] = t;
                        }
                    }
                }
            }

            async function apply_io_binding(sess, opt, feed) {
                opt.preferredOutputLocation = {};
                const kv_names = sess.outputNames.filter(n => n.includes('decoder') || n.includes('hidden_state'));
                kv_names.forEach(name => {
                    opt.preferredOutputLocation[`${name}`] = "gpu-buffer";
                });
            }

            if (config.profiler) {
                opt.enableProfiling = true;
                if (config.provider == "webgpu") {
                    ort.env.webgpu.profilingMode = 'default';
                }
                // opt.optimizedModelFilePath = 'opt.onnx';
            }

            // opt.freeDimensionOverrides = {batch_size: 1}

            log(`loading... ${config.name},  ${config.provider}`);
            const model_bytes = await fetchAndCache("models/" + config.model);
            const load_start = performance.now();
            var sess = await ort.InferenceSession.create(model_bytes, opt);
            const load_stop = performance.now();
            const feed = make_inputs(config.gen, sess.inputNames, config);

            if (config.provider == "webgpu" && config.io_binding) {
                await apply_io_binding(sess, opt, feed);
                sess = undefined;
                sess = await ort.InferenceSession.create(model_bytes, opt);
            }

            if (config.profiler) {
                log("profiling...");
                for (var i = 0; i < 10; i++) {
                    const outputs = await sess.run(clone(feed));
                    if (config.io_binding) {
                        copy_kv_caches(feed, outputs);
                    }
                }
                await sess.endProfiling();
                log("profiling done @@1.");
                return;
            }

            log("warmup...");
            await sleep(200);
            let fist_run = 0;
            {
                const start = performance.now();
                let outputs = await sess.run(clone(feed));
                fist_run = performance.now() - start;
                log(`1st run took ${fist_run.toFixed(2)}ms`);
                if (config.verbose) {
                    console.log(outputs);
                }
                for (var i = 0; i < 4; i++) {
                    outputs = await sess.run(clone(feed));
                    if (config.io_binding) {
                        copy_kv_caches(feed, outputs);
                    }
                }
            }

            log("running...");
            const durations = [];
            progress.parentNode.style.display = "block";
            await sleep(200);
            const end_time = performance.now() + config.min_query_time;
            let query_count = 0;
            let now;
            let pct_last = 0;

            do {
                const start = performance.now();
                const outputs = await sess.run(clone(feed));
                now = performance.now();
                durations.push(now - start);
                if (config.io_binding) {
                    copy_kv_caches(feed, outputs);
                }
                query_count++;
                const pct_new = Math.min((100 * query_count) / config.min_query_count, (100 - (100 * (end_time - now)) / config.min_query_time));
                if (pct_last + 5 < pct_new) {
                    pct_last = pct_new;
                    progress.style.width = pct_new.toFixed(1) + "%";
                    progress.textContent = progress.style.width;
                    await sleep(150);
                }
            } while (now < end_time || query_count < config.min_query_count);
            const r = logPercentiles(config, durations);
            r.fist_run = fist_run.toFixed(2);
            r.load = (load_stop - load_start).toFixed(2);
            r.platform = ""
            if (config.csv) {
                r.tag = "@@2";
                const csv = rows.map((x) => { return r[x]; }).join(",");
                console.log(csv);
            }
            const result = `${r.name}/${r.provider}/${r.threads} avg: ${r.avg}, min: ${r.min}, max: ${r.max}, p50: ${r.p50}, p95: ${r.p95}, p99: ${r.p99}, count=${r.count}`;
            log(result + " @@1");
            document.getElementById('results').innerText += result + "\n";
        } catch (e) {
            log(`${config.name} ${e} @@1`);
            console.log(e);
            if (config.csv) {
                log(`${config.name},${config.provider},${config.threads},0,,,,,,,,,,${e.message.replace("\n", " ")} @@2`);
            }
            if (config.profiler) {
                await sess.endProfiling();
            }
        }
        progress.parentNode.style.display = "none";
        progress.style.width = "0%";
    }
    if (config.go) {
        bench();
    }
</script>

</html>
